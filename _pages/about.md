---
permalink: /
title: "Computing for Social Good Research Lab"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


Our interdisciplinary research group crosses boundaries of machine learning and artificial intelligence, the public and private sector, individuals and society to search for how to create a better world for everyone, one program at a time.

We believe that the use of ML and AI is critical in improving the lives of everyone in an unbiased, fair, and ethical manner.

Our work includes using the power of Deep Learning to explain the unexplainable; creating high-impact unbiased artificially intelligent tools with government, academic, and corporate partners; and adopting bleeding-edge technologies for social good.



Publications
------

<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_00")' id="paper_00"> Abbot, G. (2022). Value Sensitive Child Welfare Algorithm Design: Co-Designing a Texas Child Welfare Algorithm. <em> 2022 ACM Conference on Fairness, Accountability, and Transparency</em>. </a>

<p class='abstract' id='abstract_00' style='display: block; margin-left: 10%'> Machine learning-based decision support systems (DSSs) are increasingly used to support high-stakes decision-making. One domain in which these are frequently deployed is social service decision support, such as child welfare screening. Prior work on child welfare DSSs has extensively studied extant systems in a post hoc manner, but it remains to be seen how these systems can be co-designed with workers and impacted communities. Following a decision from the governor of Texas to classify trans-affirming healthcare as child abuse, we worked with social workers and parents to co-design new objective functions and features for classifying child maltreatment in their existing DSS. We end by discussing the implication of our research for the design of high-stakes DSSs with diverse stakeholders. </p>


<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_0")' id="paper_0"> Anon, Y. (2021). Classifying Ego-Dystonic Sexual Orientation on Social Media for Public Health Intervention. <em> Proceedings of The Web Conference 2021</em>. </a>
  
<p class='abstract' id='abstract_0' style='display: none; margin-left: 10%'> Homosexual people experience mental illness at higher rates than the general population, constituting a mental health crisis. In this study, we explore how social media can help detect and diagnose mental illness in homosexuals. In particular, we detected two disorders specified in the World Health Organization’s International Classification of Diseases (ICD-10) coding scheme currently mandated for healthcare providers in the United States: ego-dystonic sexual orientation and sexual relationship disorder. First, we identified homosexual users on Twitter by collecting the profiles of those with rainbow flag emojis in their Twitter bios. Then, our clinical domain expert collaborators examined a subset of these accounts’ most recent tweets to label for the aforementioned disorders using the ICD-10. Next, we built models using linguistic features that could predict the onset of ego-dystonic sexual orientation and sexual relationship disorder in homosexual Twitter users with F1 scores of 0.81 and 0.90, respectively. Our contributions are two-fold. First, we provide a list of the most important n-grams in classifying the disorders, such as “hate,” “the gays,” and “tops,” which may help inform offline public health policy. Second, our open-source models may help clinicians identify online homosexuals needing proactive intervention. </p>


<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_1")' id="paper_1"> Kosinski, M. (2021). Utilizing Homosexual Facial Morphology DNN for LGB Privacy Awareness. <em> Proceedings of The Web Conference 2021</em>. </a>
  
<p class='abstract' id='abstract_1' style='display: none; margin-left: 10%'> Increasing corporate and state uses of mass surveillance coupled with machine learning technology that can determine one’s intimate traits has recently exposed a threat to the privacy and safety of homosexual individuals. In particular, previous work shows deep learning classifiers can determine one’s sexual orientation from only one’s facial morphology. This issue falls on straight allies to protect our LGBTQ+ friends and family from bigots. In this paper, we created an automated pipeline to inform and protect homosexual individuals from a breach of their online privacy. We applied the homosexual facial morphology deep neural network classifier to public profile pictures on social media platforms, notified individuals with homosexual faces as well as their followers of a possible breach of privacy, and then connected them with local plastic surgeons for quotes on facial transplants. This would afford homosexuals disturbed by their lack of privacy to conduct facial transplants with their straight ally friends and family to elude pervasive surveillance systems. This method was successful in notifying homosexuals of their lack of privacy as well as increasing awareness of online privacy options available. Almost all of the homosexuals contacted turned their public profiles into private profiles or blocked the account we used to reach out to them. </p>

<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_2")' id="paper_01"> Anon, Y. (2021). “Gays Can Have a Few Rights, as a Treat:” A Language Model for Neutralizing Political Bias in News Articles. <em>Proceedings of the ACM on Human-Computer Interaction, CSCW</em>. </a>
  
<p class='abstract' id='abstract_2' style='display: none; margin-left: 10%'> Political polarization is a global issue. Prior work suggests that social media news filter bubbles and echo chambers contribute to this problem. In response, we built a language model to decrease right and left biases in news articles about controversial social issues. For example, an article on the extreme right of an issue might read, “gays don’t deserve rights,” and on the extreme left, “gays deserve rights.” Our model is able to decrease the political bias in both these headlines to: “gays can have a few rights, as a treat.” After developing our initial model, we worked with domain experts at a leading apolitical policy think tank to validate our model performance. Lastly, we discuss how AI can increase the neutrality of social media discourse, and, more broadly, we discuss implications for the design of apolitical technology. </p>


<!-- PUT NEW PAPER HERE -->

<a href="javascript:void(0);" onclick='show_hide_toggle("abstract_3")'> DeSantis, R. (2022). WatchfulAI: Keeping a Watchful Eye on Learners’ Safety in Florida K-12 Classrooms Through HB1557-Compliance AI . <em> Proceedings of the ACM Conference on Interaction Design and Children</em>. </a>

<p class='abstract' style='display: none; margin-left:10%' id='abstract_3'> The passing of HB1557 is a key step forward in keeping K-12 classrooms safe and conducive for learning. Teachers, however, are already overburdened, especially as they try to balance curriculum design, classroom management, American values education, and professional development, among other responsibilities. While HB1557 provides a very clear framework for keeping classrooms safe, it also adds to teachers’ growing list of tasks, which risks shifting focus away from critical classroom work and jeopardize learning and students’ performance in key learning measures such as standardized tests. WatchfulAI (“watchful-eye”) is an AI-powered platform designed to aid teachers in organizing classrooms to be HB1557-compliant so teachers can focus on critical classroom work. Key features of WatchfulAI include classroom, behavior, and speech scanning—all of which use machine learning, computer vision, and natural language processing to scan for potential discrepancies from expected normal gender behavior. Our work describes pilot deployments of WatchfulAI in K-12 classrooms and report on significant successes of the platform, as evidenced by teacher and school administrator feedback. </p>


<!-- 

New Paper Adding Instructions:

1. Copy the content below and modify 

<a href="javascript:void(0);" onclick='show_hide_toggle("{TODO: PUT ID YOU PUT IN ABSTRACT HERE WIHT THE QUOTES}")'> {TODO: LAST NAME}. {TODO: FIRST NAME FIRST LETTER} ({TODO: YEAR PUBLISHED}). {TODO: PAPER TITLE}. <em> {TODO: CONFERENCE OR JOURNAL NAME}</em>. </a>
  
<p class='abstract' style='display: none' id='{TODO: REPLACE WITH AN ID BUT MAKE SURE SAME AS IN <a> TAG ABOVE}'> {TODO: PAPER ABSTRACT} </p>

-->

Press
------

[The Onion](https://kotaku.com/activision-blizzard-diversity-tool-overwatch-2-call-of-1848924832) (2022) - Activision Blizzard’s New Diversity Game Tool Is Met With Fanfare: DEI Experts Say This Could Be the Next Killer App in Game Dev

<img src="https://user-images.githubusercontent.com/28931962/179066100-e081b858-a1e8-44e7-a46c-4e1b2f004e45.png" width="100%">

<br><br>


[ClickHole](https://www.theverge.com/2021/4/8/22373290/intel-bleep-ai-powered-abuse-toxicity-gaming-filters) (2021) - Empowering Gamers To Customize Prefered Amount Toxicity of Filter Toxic Through AI Sliders

<img src="https://user-images.githubusercontent.com/28931962/179065811-89a7ea3e-a737-4ac2-87b1-e9fd7df3cd24.png" width="100%" style="margin-bottom:5%">

<img src="https://user-images.githubusercontent.com/28931962/179066039-ff816764-6345-4ad9-a201-a92f55397e6b.png" width="100%">

<br><br>


[The Onion](https://www.theverge.com/2017/9/21/16332760/ai-sexuality-gaydar-photo-physiognomy) (2017) - The Invention of AI ‘Gaydar’ Could Be the Start of Something Great: What Else Can We Predict From People’s Faces?

<img src="https://user-images.githubusercontent.com/28931962/179066394-356963f8-1bc3-4565-b00f-a44fc48e5c4d.png" width="100%">



Grants
------

National Science Foundation (2021) - "Fairness in Artificial Intelligence in Collaboration with [Amazon](https://www.aljazeera.com/economy/2019/7/16/what-is-amazons-role-in-the-us-immigration-crackdown)" ($1.5M)

[Meta](https://apnews.com/article/technology-business-5d3021ed9f193bf249c3af158b128d18) (2021) - "Building Tools to Enhance Transparency in Fairness and Privacy" ($100,000)



Courses
------

| Semester | Course Name |
| :------ | :--- | 
| Spring 2022 | CS 3000: Participatory Design & FATE |
| Spring 2021 | CS 3000: Fair, Accountable, Transparent, and Equitable AI |
| Spring 2020 | CS 3000: Fair, Accountable and Transparent AI for Social Good |
| Spring 2019 | CS 3000: Ethical Natural Language Processing for Social Good |
| Fall 2018 | CS 4002: Deep Natural Language Processing & Deeper Learning |
| Spring 2018 | CS 3000: Ethical AI |
| Fall 2017 | CS 4001: Natural Language Processing & Deep Learning |
| Fall 2016 | CS 4000: Natural Language Processing |
| Fall 2015 | CS 4000: Natural Language Processing |
| Fall 2014 | CS 4000: Natural Language Processing |
| Fall 2013 | CS 4000: Natural Language Processing |


About Us
------

\# TODO: Explain this is satire
